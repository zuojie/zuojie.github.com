---
layout: default
title: 机器学习实践：Logistic Regression的应用
meta: Logistic Regression, machine learning
---
# {{ page.title }}
*{{ page.date | date_to_string }}*   

##背景
####线性回归值预测是寻找一条直线拟合平面上的点，主要工作是通过不同的优化算法寻找直线的权重系数向量，以使损失/代价函数值最小。损失函数一般的度量方法是预测值和实际值的差的平方（最小二乘误差）。
####线性回归预测同样是寻找这样一条直线，不同的是某个点的坐标值代入线性方程的时候，得到的值不是预测值，而是代表对当前点的分类情况，一般如果预测值>0则分到A类，否则分到B类。这条直线是一条分界线，我们的目标是尽可能把所有点都正确归类。
####寻找最佳权重向量W的过程是一个优化过程，即不断逼近最优权重向量，使损失函数的值最小。权重的取值和损失函数的函数值存在这样一个关系：当权重值的取值向任意方向远离最优值时候，损失函数值单调增加。所以一般选择梯度下降法寻找最优解，梯度通过求导取得，表示在当前曲线的斜率，斜率为正，则当前值位于最优值的右侧，斜率为负，则当前值位于最优值的右侧，因此要想逼近最优值，就应该用当前值减去斜率（斜率一般还要乘上一个常量α，用来控制权重的变化速率）。可以预见，逼近的速度在α固定的情况下，跟斜率正相关，而斜率在W接近最优解的过程中绝对值是不断变小的（想象一下口朝上的抛物线，在x取值逼近抛物线顶点的时候，斜率绝对值的变化情况），因此可以理解为逼近速度也是逐渐放慢。一直迭代W，直到达到迭代上限或者误差小于某个阈值，迭代即可停止。
####常量α也影响迭代的最终结果，如果α取值过大，可能导致最后W不能收敛到最优解（步子太大，容易从最优解的左边一步跨到右边去）。α太小，可能会导致收敛速度很慢，自己看情况调整
####如果将回归应用到分类预测（这里指2分类问题），就不能直接使用直线方程了，因为直线方程的输出值范围是没谱的。我们拿到这个值基本对分类判断没多大帮助。所以我们期望有个回归方程，我们输入给它一坨特征，回归方程告诉我们一个分类，做到这一步目前还有困难，那么我们退而求其次，如果我们能找到一个回归方程，它告诉我们属于某个分类的概率也可以。这个回归方程就属于Logistic Regression了。
####LR一般采用sigmod函数做回归预测，sigmod函数的自变量可以为任意值，因变量取值范围为(0,1)，自变量为0时因变量为0.5。LR中我们不再直接使用分界线方程，而是将分界线方程的值作为sigmod函数的输入，从而得到分类的概率值，这样就把特征值和分类结果间接的关联起来了。
####有了回归候选人，下面就是确定训练它的方法，同样还是损失函数。但是这次不再使用最小二乘误差来度量损失，因为我们的实际值只有0和1两种（2分类问题嘛）。我们期望损失函数具有几点性质：由于sigmod的输出值范围为(0,1),所以当输出值范围在0到0.5时，我们将其归类为0，范围为0.5到1时，归类为1，所以我们期望当实际分类为0时，sigmod的输出值和损失要成反比（输出值越大，损失越大，输出值接近1的时候说明判断错误，损失值达到最大）；相反，如果实际值为1时，那么我们希望sigmod的输出值和损失成正比。于是有人YY出了这个神奇的公式：   
![st](/demo/LR1.png)   
#####显然，能满足我们的要求。进一步调整，上述式子的等价形式为：   
![st](/demo/LR2.png)   
#####而求权重向量的方法仍然是梯度下降。有趣的是，线性回归和LR在迭代求解W的时候，权重的变化公式是一样的。   

##实战
