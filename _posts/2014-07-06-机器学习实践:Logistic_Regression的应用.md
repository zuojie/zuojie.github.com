---
layout: default
title: 机器学习实践：Logistic Regression的应用
meta: Logistic Regression, machine learning
---
# {{ page.title }}
*{{ page.date | date_to_string }}*   

##背景
####线性回归值预测是寻找一条直线拟合平面上的点，主要工作是通过不同的优化算法寻找直线的权重系数向量，以使损失/代价函数值最小。从而尽量准确预测未来的一些数据，损失函数一般的度量方法是预测值和实际值的差的平方（最小二乘误差）。
####寻找最佳权重向量W的过程是一个优化过程，即不断逼近最优权重向量，使损失函数的值最小。权重的取值和损失函数的函数值存在这样一个关系：当权重值的取值向任意方向远离最优值时候，损失函数值单调增加。所以一般选择梯度下降法寻找最优解，梯度通过求导取得，表示在当前曲线的斜率，斜率为正，则当前值位于最优值的右侧，斜率为负，则当前值位于最优值的右侧，因此要想逼近最优值，就应该用当前值减去斜率（斜率一般还要乘上一个常量α，用来控制权重的变化速率）。可以预见，逼近的速度在α固定的情况下，跟斜率正相关，而斜率在W接近最优解的过程中绝对值是不断变小的（想象一下口朝上的抛物线，在x取值逼近抛物线顶点的时候，斜率绝对值的变化情况），因此可以理解为逼近速度也是逐渐放慢。一直迭代W，直到达到迭代上限或者误差小于某个阈值，迭代即可停止。
####常量α也影响迭代的最终结果，如果α取值过大，可能导致最后W不能收敛到最优解（步子太大，容易从最优解的一边一步跨到另一边去）。α太小，可能会导致收敛速度很慢，自己看情况调整
####如果将回归应用到分类预测（这里指2分类问题），就不能直接使用直线方程了，因为直线方程的输出值范围是没谱的。我们拿到这个值基本对分类判断没多大帮助。所以我们期望有个回归方程，我们喂给它一坨特征，回归方程吐给我们一个分类，做到这一步目前还有困难，那么我们退而求其次，如果我们能找到一个回归方程，它告诉我们属于某个分类的概率也可以。这个回归方程便是Logistic Regression了。
####LR一般采用sigmod函数做回归预测，sigmod函数的自变量可以为任意值，因变量取值范围为(0,1)，自变量为0时因变量为0.5。LR中我们不再直接使用分界线方程，而是将分界线方程的值作为sigmod函数的输入，从而得到分类的概率值，这样就把特征值和分类结果间接的关联起来了。
####有了回归候选方程，下面就是确定训练它的方法，同样还使用是损失函数度量准确度。但是这次不再使用最小二乘误差来度量损失，因为我们的实际值只有0和1两种（2分类问题嘛）。我们期望损失函数具有几点性质：由于sigmod的输出值范围为(0,1),所以当输出值范围在0到0.5时，我们将其归类为0，范围为0.5到1时，归类为1，所以我们期望当实际分类为0时，sigmod的输出值和损失要成反比（输出值越大，损失越大，输出值接近1的时候说明判断错误，损失值达到最大）；相反，如果实际值为1时，那么我们希望sigmod的输出值和损失成正比。于是有人提出了这个神奇的公式：   
![st](/demo/LR1.png)   
#####显然，能满足我们的要求。进一步调整，上述式子的等价形式为（这是最大似然（likehood，翻译成可能性更直观吧）函数的相反数形式：姑且称其为是最小似然，相应的要用梯度下降算法来优化求得最小似然值。稍微延伸一下，似然函数的值表达的是关于函数中的未知量在取某些值时，造成当前现状的可能性。比如在预测问题中，权重系数向量是未知的，特征值是已知的。用上述拐弯抹角的方式表达就是：损失函数<也就是似然函数>的值表达的是，权重系数取某些值的时候，根据当前的特征值得到最终分类属于m（0或者1）的可能性。看到这应该有人烦了，权重系数到底是哪些值？其实似然函数也不知道是哪些值，于是只好求那些使似然函数值<这里也就是损失函数值的相反数>最大的权重系数值，也就是最大似然性问题。关于似然函数可参考下面的引用部分，讲的很好）：   
![st](/demo/LR2.png)   
#####而求权重向量的方法仍然是梯度下降。有趣的是，线性回归和LR在迭代求解W的时候，权重的变化公式是一样的。   
![st](/demo/LR3.png)      
#####化简变形（这也是线性回归情况下梯度下降方法的权重变化公式）
![st](/demo/LR4.png)
####提前放出python代码，上述迭代公式对应的python代码如下（已经使用numpy进行vectorization）
![st](/demo/LR5.png)   

##实战
实战直接使用《MIA》提供的数据集，每一行格式如下：   
x y label   
分别为x，y坐标，分类（0 or 1）.
 
使用剃度下降跑500圈的效果如图：   
![st](/demo/LR6_500.png)   
使用随机梯度下降跑100圈如图：   
![st](/demo/LR7_100.png)   
使用随机梯度下降跑500圈如图：   
![st](/demo/LR7_500.png)   

代码见这里[LR](https://github.com/zuojie/MachineLearningCases/tree/master/LR)    
###参考资料
* [从最大似然到EM算法浅解](http://blog.csdn.net/zouxy09/article/details/8537620)
* 《Machine Learning in Action》
* Andrew NG《Machine Learning》
